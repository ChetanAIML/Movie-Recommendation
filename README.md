# Movie-Recommendation

## Solution Overview

Collaborative filtering is a powerful recommendation technique used in recommendation systems to provide personalized suggestions to users based on their historical interactions and the preferences of similar users. It operates on the principle that users who have shown similar behaviors in the past are likely to have similar tastes and preferences in the future. To implement collaborative filtering, a key step involves creating a user-item matrix, which captures user-item interactions. Similarity measures, such as cosine similarity or matrix factorization, are then used to identify users or items with comparable preferences. Recommendations are generated by suggesting items that similar users have engaged with, but the target user has not, leading to a more tailored and relevant user experience. Collaborative filtering has proven effective in various domains, from e-commerce and movie streaming platforms to social media and news aggregators, enhancing user engagement and satisfaction. The solution involves leveraging a pre-trained model from “hugging face” to recommend movies. The “movies.csv” dataset from Kaggle is used as a test dataset, in addition to recommending movies for the user. 

![Picture1](https://github.com/ChetanAIML/Movie-Recommendation/assets/83413651/a586a77e-de2e-489a-9f77-2cf81c584d56)

## Data Sources

The main data source for this project is the “Movies Daily Update Dataset” from Kaggle (https://www.kaggle.com/datasets/akshaypawar7/millions-of-movies) , which contains over 500k unique movies with various genres like “Documentary”, “Drama”, “Action”, Comedy”, “Music” etc. This dataset provides cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages, reviews, recommendations. We will use this dataset to generate a user-item matrix for collaborative filtering. 

## Data Engineering:

We stored the “movies.csv” dataset from “Kaggle” into an amazon S3 bucket. We then created an Amazon Athena database to store the dataset in a table in Athena database. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. 
Exploratory Data Analysis (EDA) was done on the dataset to remove duplicates (duplicate movie records) to bring the row count close to 600k records. Other pre-processing techniques were used to clean up the dataset. Like taking care of the Null values in columns, imbalance in datasets etc. 

## Training Data:

To recommend movies, we leveraged a pre-trained “HuggingFace” model (https://huggingface.co/sentence-transformers/all-mpnet-base-v2). The “movies” dataset was used as “test” dataset for prediction. The pre-trained model concatenated data from multiple datasets like “Reddit comments”, “S2ORC”, “WikiAnswers”, “Stack Exchange” etc. to fine-tune its model using a contrastive objective.  The total number of sentence pairs is above 1 billion sentences.

## Feature Engineering:

As we are leveraging a customized pre-trained model for prediction, we further tweaked the pre-processed dataset to create appropriate input for the model. The input can be considered as the “test” dataset. Feature engineering included filtering the “movies” dataset to get movies that contain at least “20” vote counts.  Data manipulation was done against “genres”, “keywords” and “credits” columns to replace “hyphens” with “spaces”. Feature engineering was done to the “credits” column to extract the first five credits (e.g., actors, directors, producers) while removing spaces and replacing hyphens with spaces. Finally, a custom “DataFrame” called “tags” was created that combines information from the 'overview,' 'genres,' 'keywords,' 'credits,' and 'original_language' columns into a single string, separated by spaces. This 'tags' column is used as the basis for content-based recommendation.

## Text Processing & Vectorization:

For movie recommendation, the feature engineered dataset is further post-processed to make it suitable for prediction. This step can be considered as a part of the “client” processing step. For instance, sample rows (say 50 records) are extracted from the “tags” DataFrame and stored in a list. This is then applied against a “stemming” function that applies stemming to a given text by splitting the text into words and stemming each word, then joining them back. A TF-IDX matrix is then computed against the “tags” column that represents the “textual content” for each movie.

## Model Prediction:

A custom function called “get_recommendations” is written that takes a movie title as input, computes cosine similarity between the TF-IDF matrix and the input and returns the top 10 most similar movies to the input movie title. 

## Model Training & Evaluation:

To recommend movies, we leveraged a pre-trained “HuggingFace” model. We used the “all-mpnet-base-v2” sentence-transformer model from “Hugging Face”. This model maps sentences and paragraphs to a 768-dimensional dense vector space. In short, the model is intended to be used as a sentence and short paragraph encoder. However, to predict, we used the model without “sentence-transformers”. This was achieved by passing the input (text) through the transformer model, prior to applying the right pooling-operation on-top of the contextualized word embeddings. Given an input text, it outputs a vector which captures the semantic information. This sentence vector is used for information retrieval against the pre-processed / feature engineered movies dataset to help recommend movies for users. The evaluation for the pre-trained model can be found here - https://www.sbert.net/_static/html/models_en_sentence_embeddings.html

## Model Deployment:

Before deploying the model, an instance of the “HuggingFaceModel” class is created with parameters like “model_data”, “role”, “transformers_version”, “pytorch_version” and “py_version”. Model is deployed to the endpoint via the deploy method with an “initial_instance_count” of 1 and an “instance_type” of “ml.g4dn.xlarge” for computational resources available for inference. You can send input data to the endpoint and receive model predictions in response.

## Model Monitoring:

We are using SageMaker ModelQualityMonitor to monitor our model performance and output with the baseline version of data we have created during model prediction stage.

## Model CI/CD:

Amazon SageMaker Pipelines is used to deploy a pre-trained Hugging Face Transformer model. The SageMaker integration with Hugging Face makes it easy to train and deploy advanced NLP models. A Lambda step in SageMaker Pipelines enables you to easily do lightweight model deployments and other serverless operations.
The pre-trained model is registered in the Model Registry under a Model Package Group. Each time a new model is registered, it is given a new version number by default. The model is registered in the "Approved" state so that it can be deployed. A custom ModelDeployment derived from the provided LambdaStep helps deploy model to SageMaker Endpoint.
A SageMaker pipeline is created that does the above 2 steps (Register Hugging Face Model & Deploy Hugging Face Model)
